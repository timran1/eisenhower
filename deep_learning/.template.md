# Deep Learning Feature Trees

This page outlines Backend Development Cost (BDC) analysis of Deep Learning accelerators. The Feature Tree Template (\<FT>) used for this analysis is available [below](#feature-tree-template). We used the provided \<FT> to model the following hardware:
* Unicore RISC-V CPU
* Intel Xeon CPU
* NVIDIA Fermi GPU
* NVIDIA Turing GPU
* Google TPU v1
* Versatile Tensor Accelerator (VTA)
* Intel Spring Hill
* NVIDIA Simba

## Feature Tree Template (\<FT>)

<table_placeholder>

## Backend Development Stages
TODO: Put diagram. discuss

## \<FT> Details
We now provide more details for the hardware features discussed in \<FT> above.

### Organization
The <FT> organizes Hardware Features Descriptors (HDFs) into following categories:
* Memory
* Node
    * Datapaths
    * Control
    * Data Movement
* Network and Synchronization

We differentiate between nodes and datapaths as follows. TODO: describe.

### Memory Hierarchy : Implicit Data Movement
This feature captures the number of cache levels in the memory hierarchy. Presence of cache is transparent to developers at functional stages. However, it is imperative for performance to employ loop tilling/blocking such that inner-most loop levels access data residing in closest cache levels.

### Memory Hierarchy : Explicit Data Movement
This feature captures the number of scratch levels in the memory hierarchy. Presence of scratchpads require developer attention even for the first functional version of backend. Hence, cache incurs BDC at all stages. 

### Memory Hierarchy : Software Coherency
Some levels of the memory hierarchy may delegate coherency management to the developer. The developer must ensure that multiple nodes accessing the same memory location can view the latest value. As cohrency is only relevant for multi-node scenarios, this feature incurs BDC at distributed backend development stages only.

### Memory Hierarchy : Data Access Granularity
Different memory levels may have different memory lines access granularity. While typically the memory line (or cache line) size is same throughout the memory hierarchy (order of bytes), modern high-capacity memories may present a bigger memory line size (order of few kilobytes). This mismatch may require additional considerations for optimization stages.

TODO: Explain diagram

### Memory Hierarchy : Storage Properties
Modern accelerators may specialize memory units to store only particular kinds of data. For example, Deep Learning accelerators dedicate some memory units to store activations while other memory units just to store weights. We use "Storage Properties" to refer to such memory specialization (which may extend beyond just weights or activations) as opposed to a homogenous memory unit. For the general case of operators which do not have a notion of weights and activations (such as concatanetaion), these storage properties require additional developer considerations during data placement and incur BDC at all stages of backend developement.

TODO: Explain diagram

### Node Types Set : Node : Data Movement : DMA Engines
Presence of DMA engines in a node opens up the possibility of an array of data movement scenarios concurrent with the node operation. Effectively using the DMAs requires additional developer considerations at optimization stages.

### Node Types Set : Node : Data Movement : Stride
Data movement problems can be formulated in different ways depending on the number of dimensions of stride supported by hardware. Higher number of stride dimensions increase the number of ways a data movement problem can be mapped to a data movement primitives. At the simplest extreme, no stride forces the developer to issue contiguous data movement commands. It is relevant to note that development for some operators working on high dimensional data may be eased by a higher stride dimension. Stride affects all single-node stages of backend development.

TODO: Explain diagram

### Node Types Set : Node : Data Movement : Patterns
Every multi-node system at least supports a peer-to-peer data movement primitive. More advanced patterns such as broadcast (multicast) or scatter/gather may also be supported by hardware. The number of possible formulations of a data movement problem can increase with the number of hardware supported data movement patterns. This feature is relevant in multi-node systems and impacts both distributed stages of backend development.

### Node Types Set : Node : Control : Indirect Addressing
If the nodes in a multi-node system support indirect addressing for memory access, the same instruction stream (indexed to appropriate data using node ID) can be executed at all the nodes involved. This can greatly simplify deployment of kernels across the distributed system of nodes. Indirect addressing also reduces the badnwidth requirements for instruction stream delivery to all nodes. This factor impacts the *func-dist* stage of backend development.

### Node Types Set : Node : Control : Loop Levels
Deep Learning accelerators may limit the control flow primitives supported at N inner-most loop levels for implementation simplicity and performance (reduced instruction stream bandwidth). This optimization requires the developer to formulate all instruction streams as at least N nested loops for best hardware utilization. This feature incurs BDC at *opt-single* stage.

### Node Types Set : Node : Control : Latency Hiding
Control at nodes may expose software managed latency hiding capabilities critical to ensure maximum hardware utilization. Some of the examples include hardware contexts for multithreading and software-managed double buffering. These latency hiding techniques are relevant at both optimization stages. 

Each of these latency hiding techniques interact with each other in complex ways which incurs a high BDC for full hardware utilizaiton. For example, implementing a multi-threaded implementation requires special developer effort; furthermore, properly managing double buffers in a multithreaded implementations is an even more daunting task. Hence, the BDC for this feature scales exponentially with *num_dims*.

### Node Types Set : Node : Control : Data Dependency


### Node Types Set : Node : Control : ISA Specialization


### Node Types Set : Node : Datapaths Set : Datapath : Operation Dimensions


### Node Types Set : Node : Datapaths Set : Datapath : Unmaskable Dimensions

### Node Types Set : Node : Datapaths Set : Datapath : Memory Levels


### Node Types Set : Node : Datapaths Set : Datapath : Latency Hiding

### Network and Synchronization : Latency
TODO: Per feature details here


### Network and Synchronization : Bandwidth


### Network and Synchronization : Topology Positions


### Network and Synchronization : Sync Capability


## References
TODO: Add
